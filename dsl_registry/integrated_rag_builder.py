#!/usr/bin/env python3
"""
GraphQLê³¼ OpenAPI DSLì„ í†µí•©í•˜ì—¬ RAG ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import json
import yaml
from pathlib import Path
from typing import Dict, List, Any, Optional

try:
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_community.embeddings import OpenAIEmbeddings
    from langchain_community.vectorstores import Chroma
    from langchain.schema import Document
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("âš ï¸ LangChainì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¬¸ì„œ ìƒì„±ë§Œ ì§„í–‰í•©ë‹ˆë‹¤.")

class IntegratedRAGBuilder:
    """í†µí•© RAG ì¸ë±ìŠ¤ ë¹Œë”"""
    
    def __init__(self, openai_api_key: str = None):
        self.openai_api_key = openai_api_key
        
        if LANGCHAIN_AVAILABLE and openai_api_key:
            self.embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                separators=["\n\n", "\n", " ", ""]
            )
        else:
            self.embeddings = None
            self.text_splitter = None
    
    def load_graphql_dsl(self, dsl_dir: str) -> List[Dict[str, Any]]:
        """GraphQL DSL íŒŒì¼ë“¤ì„ ë¡œë“œ"""
        documents = []
        dsl_path = Path(dsl_dir)
        
        # DSL JSON íŒŒì¼ë“¤ ì²˜ë¦¬
        for json_file in dsl_path.glob("*.json"):
            with open(json_file, 'r', encoding='utf-8') as f:
                dsl_data = json.load(f)
            
            # DSL ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            content = self._dsl_to_text(dsl_data, "GraphQL")
            metadata = {
                'source': str(json_file),
                'type': 'graphql',
                'dsl_name': dsl_data.get('name', 'Unknown')
            }
            
            documents.append({
                'content': content,
                'metadata': metadata
            })
        
        # DSL YAML íŒŒì¼ë“¤ ì²˜ë¦¬
        for yaml_file in dsl_path.glob("*.yaml"):
            with open(yaml_file, 'r', encoding='utf-8') as f:
                dsl_data = yaml.safe_load(f)
            
            # DSL ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            content = self._dsl_to_text(dsl_data, "GraphQL")
            metadata = {
                'source': str(yaml_file),
                'type': 'graphql',
                'dsl_name': dsl_data.get('name', yaml_file.stem)
            }
            
            documents.append({
                'content': content,
                'metadata': metadata
            })
        
        # DSL ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë“¤ ì²˜ë¦¬
        for md_file in dsl_path.glob("*.md"):
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            metadata = {
                'source': str(md_file),
                'type': 'graphql',
                'dsl_name': md_file.stem
            }
            
            documents.append({
                'content': content,
                'metadata': metadata
            })
        
        return documents
    
    def load_openapi_dsl(self, dsl_dir: str) -> List[Dict[str, Any]]:
        """OpenAPI DSL íŒŒì¼ë“¤ì„ ë¡œë“œ"""
        documents = []
        dsl_path = Path(dsl_dir)
        
        # ë©”ì¸ DSL JSON íŒŒì¼ ì²˜ë¦¬
        main_dsl_file = dsl_path / "openapi_dsl.json"
        if main_dsl_file.exists():
            with open(main_dsl_file, 'r', encoding='utf-8') as f:
                dsl_data = json.load(f)
            
            content = self._dsl_to_text(dsl_data, "OpenAPI")
            metadata = {
                'source': str(main_dsl_file),
                'type': 'openapi',
                'dsl_name': dsl_data.get('name', 'OpenAPI Service')
            }
            
            documents.append({
                'content': content,
                'metadata': metadata
            })
        
        # ë©”ì¸ DSL YAML íŒŒì¼ ì²˜ë¦¬
        main_dsl_yaml = dsl_path / "openapi_dsl.yaml"
        if main_dsl_yaml.exists():
            with open(main_dsl_yaml, 'r', encoding='utf-8') as f:
                dsl_data = yaml.safe_load(f)
            
            content = self._dsl_to_text(dsl_data, "OpenAPI")
            metadata = {
                'source': str(main_dsl_yaml),
                'type': 'openapi',
                'dsl_name': dsl_data.get('name', 'OpenAPI Service')
            }
            
            documents.append({
                'content': content,
                'metadata': metadata
            })
        
        # ê°œë³„ ì‘ì—… íŒŒì¼ë“¤ ì²˜ë¦¬ (YAML)
        operations_dir = dsl_path / "operations"
        if operations_dir.exists():
            for op_file in operations_dir.glob("*.yaml"):
                with open(op_file, 'r', encoding='utf-8') as f:
                    op_data = yaml.safe_load(f)
                
                content = self._operation_to_text(op_data)
                metadata = {
                    'source': str(op_file),
                    'type': 'openapi',
                    'operation': op_data.get('operationId', op_file.stem)
                }
                
                documents.append({
                    'content': content,
                    'metadata': metadata
                })
        
        # ê°œë³„ ì‘ì—… íŒŒì¼ë“¤ ì²˜ë¦¬ (ë§ˆí¬ë‹¤ìš´)
        if operations_dir.exists():
            for op_file in operations_dir.glob("*.md"):
                with open(op_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                metadata = {
                    'source': str(op_file),
                    'type': 'openapi',
                    'operation': op_file.stem
                }
                
                documents.append({
                    'content': content,
                    'metadata': metadata
                })
        
        return documents
    
    def _dsl_to_text(self, dsl_data: Dict[str, Any], dsl_type: str) -> str:
        """DSL ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        lines = []
        
        if dsl_type == "GraphQL":
            lines.append(f"# {dsl_data.get('name', 'GraphQL Schema')}")
            lines.append(f"Description: {dsl_data.get('description', '')}")
            
            # ì¿¼ë¦¬ ì •ë³´
            if 'queries' in dsl_data:
                lines.append("\n## Queries:")
                for query_name, query_info in dsl_data['queries'].items():
                    lines.append(f"- {query_name}: {query_info.get('type', '')}")
                    if query_info.get('description'):
                        lines.append(f"  Description: {query_info['description']}")
            
            # ë®¤í…Œì´ì…˜ ì •ë³´
            if 'mutations' in dsl_data:
                lines.append("\n## Mutations:")
                for mutation_name, mutation_info in dsl_data['mutations'].items():
                    lines.append(f"- {mutation_name}: {mutation_info.get('type', '')}")
                    if mutation_info.get('description'):
                        lines.append(f"  Description: {mutation_info['description']}")
            
            # íƒ€ì… ì •ë³´
            if 'types' in dsl_data:
                lines.append("\n## Types:")
                for type_name, type_info in dsl_data['types'].items():
                    lines.append(f"- {type_name}")
                    if type_info.get('description'):
                        lines.append(f"  Description: {type_info['description']}")
        
        elif dsl_type == "OpenAPI":
            lines.append(f"# {dsl_data.get('name', 'OpenAPI Service')}")
            lines.append(f"Description: {dsl_data.get('description', '')}")
            lines.append(f"Base URL: {dsl_data.get('baseUrl', '')}")
            
            # ì‘ì—… ì •ë³´
            if 'operations' in dsl_data:
                lines.append("\n## Operations:")
                for operation in dsl_data['operations']:
                    lines.append(f"- {operation.get('operationId', 'Unknown')}")
                    lines.append(f"  Method: {operation.get('method', '')}")
                    lines.append(f"  Path: {operation.get('path', '')}")
                    if operation.get('description'):
                        lines.append(f"  Description: {operation['description']}")
                    
                    # íŒŒë¼ë¯¸í„° ì •ë³´
                    if operation.get('parameters'):
                        lines.append("  Parameters:")
                        for param in operation['parameters']:
                            required = "Required" if param.get('required', False) else "Optional"
                            lines.append(f"    - {param['name']} ({param.get('type', 'string')}, {required})")
            
            # íƒœê·¸ ì •ë³´
            if 'tags' in dsl_data:
                lines.append("\n## Tags:")
                for tag_name, tag_info in dsl_data['tags'].items():
                    lines.append(f"- {tag_name}: {tag_info.get('description', '')}")
        
        return '\n'.join(lines)
    
    def _operation_to_text(self, operation_data: Dict[str, Any]) -> str:
        """ê°œë³„ OpenAPI ì‘ì—…ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        lines = []
        
        lines.append(f"# {operation_data.get('operationId', 'Unknown Operation')}")
        lines.append(f"Method: {operation_data.get('method', '')}")
        lines.append(f"Path: {operation_data.get('path', '')}")
        
        if operation_data.get('summary'):
            lines.append(f"Summary: {operation_data['summary']}")
        
        if operation_data.get('description'):
            lines.append(f"Description: {operation_data['description']}")
        
        if operation_data.get('tags'):
            lines.append(f"Tags: {', '.join(operation_data['tags'])}")
        
        # íŒŒë¼ë¯¸í„° ì •ë³´
        if operation_data.get('parameters'):
            lines.append("\n## Parameters:")
            for param in operation_data['parameters']:
                required = "Required" if param.get('required', False) else "Optional"
                lines.append(f"- {param['name']} ({param.get('type', 'string')}, {required})")
                if param.get('description'):
                    lines.append(f"  Description: {param['description']}")
        
        # ìš”ì²­ ë³¸ë¬¸
        if operation_data.get('requestBody'):
            lines.append("\n## Request Body:")
            if operation_data['requestBody'].get('contentType'):
                lines.append(f"Content-Type: {operation_data['requestBody']['contentType']}")
            
            if operation_data['requestBody'].get('properties'):
                lines.append("Properties:")
                for prop_name, prop_info in operation_data['requestBody']['properties'].items():
                    lines.append(f"- {prop_name} ({prop_info.get('type', 'object')})")
                    if prop_info.get('description'):
                        lines.append(f"  Description: {prop_info['description']}")
        
        # ì‘ë‹µ ì •ë³´
        if operation_data.get('responses'):
            lines.append("\n## Responses:")
            for status_code, response in operation_data['responses'].items():
                lines.append(f"- {status_code}: {response.get('description', '')}")
        
        return '\n'.join(lines)
    
    def build_integrated_index(self, graphql_dsl_dir: str, openapi_dsl_dir: str, output_dir: str):
        """í†µí•© RAG ì¸ë±ìŠ¤ êµ¬ì¶•"""
        print("GraphQL DSL ë¡œë“œ ì¤‘...")
        graphql_docs = self.load_graphql_dsl(graphql_dsl_dir)
        print(f"GraphQL ë¬¸ì„œ: {len(graphql_docs)}ê°œ")
        
        print("OpenAPI DSL ë¡œë“œ ì¤‘...")
        openapi_docs = self.load_openapi_dsl(openapi_dsl_dir)
        print(f"OpenAPI ë¬¸ì„œ: {len(openapi_docs)}ê°œ")
        
        # ëª¨ë“  ë¬¸ì„œ í†µí•©
        all_docs = graphql_docs + openapi_docs
        print(f"ì´ ë¬¸ì„œ: {len(all_docs)}ê°œ")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # ë¬¸ì„œë¥¼ íŒŒì¼ë¡œ ì €ì¥ (RAG ì¸ë±ìŠ¤ ëŒ€ì‹ )
        print("ë¬¸ì„œ ì €ì¥ ì¤‘...")
        for i, doc in enumerate(all_docs):
            doc_file = output_path / f"document_{i:03d}.json"
            with open(doc_file, 'w', encoding='utf-8') as f:
                json.dump(doc, f, indent=2, ensure_ascii=False)
        
        # í†µê³„ ì •ë³´ ì €ì¥
        stats = {
            'total_documents': len(all_docs),
            'graphql_documents': len(graphql_docs),
            'openapi_documents': len(openapi_docs),
            'sources': {
                'graphql': [doc['metadata']['source'] for doc in graphql_docs],
                'openapi': [doc['metadata']['source'] for doc in openapi_docs]
            }
        }
        
        with open(output_path / 'index_stats.json', 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… í†µí•© ë¬¸ì„œ ì €ì¥ ì™„ë£Œ!")
        print(f"ì €ì¥ ìœ„ì¹˜: {output_dir}")
        print(f"í†µê³„ ì •ë³´ ì €ì¥: {output_path / 'index_stats.json'}")
        
        # LangChainì´ ì‚¬ìš© ê°€ëŠ¥í•˜ê³  API í‚¤ê°€ ìˆìœ¼ë©´ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        if LANGCHAIN_AVAILABLE and self.openai_api_key:
            print("ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
            self._create_vectorstore(all_docs, output_path)
        
        return stats
    
    def _create_vectorstore(self, documents: List[Dict[str, Any]], output_path: Path):
        """ë²¡í„° ìŠ¤í† ì–´ ìƒì„± (LangChain ì‚¬ìš©)"""
        # Document ê°ì²´ë¡œ ë³€í™˜
        langchain_docs = []
        for doc in documents:
            langchain_docs.append(Document(
                page_content=doc['content'],
                metadata=doc['metadata']
            ))
        
        # ë¬¸ì„œ ë¶„í• 
        split_docs = []
        for doc in langchain_docs:
            splits = self.text_splitter.split_documents([doc])
            split_docs.extend(splits)
        
        print(f"ë¶„í• ëœ ë¬¸ì„œ: {len(split_docs)}ê°œ")
        
        # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        vectorstore = Chroma.from_documents(
            documents=split_docs,
            embedding=self.embeddings,
            persist_directory=str(output_path / "chroma")
        )
        
        # ì¸ë±ìŠ¤ ì €ì¥
        vectorstore.persist()
        print("âœ… ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ!")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import os
    import yaml
    
    # ì„¤ì • íŒŒì¼ ë¡œë“œ
    config_path = "config/settings.yaml"
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # OpenAI API í‚¤ í™•ì¸ (ì„¤ì • íŒŒì¼ì—ì„œ ìš°ì„ , í™˜ê²½ë³€ìˆ˜ëŠ” ë°±ì—…)
    openai_api_key = config.get('llm', {}).get('openai_api_key') or os.getenv('OPENAI_API_KEY')
    
    # ê²½ë¡œ ì„¤ì • - ì„¤ì • íŒŒì¼ì—ì„œ ì½ê¸°
    graphql_dsl_dir = "generated_dsl/graphql_dsl"
    openapi_dsl_dir = "generated_dsl/openapi_dsl"
    output_dir = config.get('chroma', {}).get('persist_directory', 'rag/chroma_db')
    
    print(f"ì„¤ì • íŒŒì¼ ë¡œë“œ: {config_path}")
    print(f"OpenAI API í‚¤: {'ì„¤ì •ë¨' if openai_api_key else 'ì—†ìŒ'}")
    print(f"ChromaDB ê²½ë¡œ: {output_dir}")
    
    # RAG ë¹Œë” ì´ˆê¸°í™”
    builder = IntegratedRAGBuilder(openai_api_key)
    
    # í†µí•© ì¸ë±ìŠ¤ êµ¬ì¶•
    stats = builder.build_integrated_index(graphql_dsl_dir, openapi_dsl_dir, output_dir)
    
    print(f"\nğŸ“Š ìµœì¢… í†µê³„:")
    print(f"   ì´ ë¬¸ì„œ: {stats['total_documents']}ê°œ")
    print(f"   GraphQL ë¬¸ì„œ: {stats['graphql_documents']}ê°œ")
    print(f"   OpenAPI ë¬¸ì„œ: {stats['openapi_documents']}ê°œ")

if __name__ == "__main__":
    main() 