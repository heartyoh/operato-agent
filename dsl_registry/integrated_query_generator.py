#!/usr/bin/env python3
"""
GraphQLê³¼ OpenAPIë¥¼ ëª¨ë‘ ì§€ì›í•˜ëŠ” í†µí•© ì¿¼ë¦¬ ìƒì„±ê¸°
"""

import json
import yaml
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import Document

class IntegratedQueryGenerator:
    """í†µí•© ì¿¼ë¦¬ ìƒì„±ê¸°"""
    
    def __init__(self, openai_api_key: str, rag_index_path: str):
        self.embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
        self.llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.1,
            openai_api_key=openai_api_key
        )
        
        # RAG ì¸ë±ìŠ¤ ë¡œë“œ
        self.vectorstore = Chroma(
            persist_directory=rag_index_path,
            embedding_function=self.embeddings
        )
        
        # í”„ë¡œí† ì½œ ê°ì§€ í”„ë¡¬í”„íŠ¸
        self.protocol_detection_prompt = ChatPromptTemplate.from_template("""
ë‹¤ìŒ ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì–´ë–¤ API í”„ë¡œí† ì½œ(GraphQL ë˜ëŠ” REST/OpenAPI)ì´ ì í•©í•œì§€ íŒë‹¨í•˜ì„¸ìš”.

ì‚¬ìš©ì ìš”ì²­: {user_query}

íŒë‹¨ ê¸°ì¤€:
- GraphQL: ë³µì¡í•œ ë°ì´í„° ì¡°íšŒ, ì¤‘ì²©ëœ ê´€ê³„, ë‹¨ì¼ ì—”ë“œí¬ì¸íŠ¸, íƒ€ì… ì‹œìŠ¤í…œ í™œìš©
- REST/OpenAPI: ê°„ë‹¨í•œ CRUD ì‘ì—…, ë¦¬ì†ŒìŠ¤ ì¤‘ì‹¬, í‘œì¤€ HTTP ë©”ì„œë“œ

ì‘ë‹µ í˜•ì‹:
```json
{{
  "protocol": "graphql|rest",
  "reasoning": "ì„ íƒ ì´ìœ  ì„¤ëª…",
  "confidence": 0.0-1.0
}}
```
""")
        
        # GraphQL ì¿¼ë¦¬ ìƒì„± í”„ë¡¬í”„íŠ¸
        self.graphql_prompt = ChatPromptTemplate.from_template("""
ë‹¤ìŒ GraphQL ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ìš”ì²­ì— ë§ëŠ” GraphQL ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.

GraphQL ìŠ¤í‚¤ë§ˆ ì •ë³´:
{context}

ì‚¬ìš©ì ìš”ì²­: {user_query}

ìš”êµ¬ì‚¬í•­:
1. ìœ íš¨í•œ GraphQL ì¿¼ë¦¬ êµ¬ë¬¸ ì‚¬ìš©
2. í•„ìš”í•œ í•„ë“œë§Œ ì„ íƒ
3. ì ì ˆí•œ ì¸ì ì‚¬ìš©
4. ì—ëŸ¬ ì²˜ë¦¬ ê³ ë ¤

ìƒì„±ëœ GraphQL ì¿¼ë¦¬:
""")
        
        # REST API ìš”ì²­ ìƒì„± í”„ë¡¬í”„íŠ¸
        self.rest_prompt = ChatPromptTemplate.from_template("""
ë‹¤ìŒ OpenAPI ìŠ¤í™ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ìš”ì²­ì— ë§ëŠ” REST API ìš”ì²­ì„ ìƒì„±í•˜ì„¸ìš”.

OpenAPI ìŠ¤í™ ì •ë³´:
{context}

ì‚¬ìš©ì ìš”ì²­: {user_query}

ìš”êµ¬ì‚¬í•­:
1. ì ì ˆí•œ HTTP ë©”ì„œë“œ ì„ íƒ
2. ì˜¬ë°”ë¥¸ ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©
3. í•„ìš”í•œ í—¤ë” í¬í•¨
4. ìš”ì²­ ë³¸ë¬¸ êµ¬ì„± (í•„ìš”ì‹œ)
5. ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì‚¬ìš© (í•„ìš”ì‹œ)

ìƒì„±ëœ REST API ìš”ì²­:
""")
    
    def detect_protocol(self, user_query: str) -> Dict[str, Any]:
        """ì‚¬ìš©ì ìš”ì²­ì— ì í•©í•œ í”„ë¡œí† ì½œ ê°ì§€"""
        response = self.llm.invoke(
            self.protocol_detection_prompt.format(user_query=user_query)
        )
        
        try:
            result = json.loads(response.content)
            return result
        except json.JSONDecodeError:
            # ê¸°ë³¸ê°’ìœ¼ë¡œ GraphQL ë°˜í™˜
            return {
                "protocol": "graphql",
                "reasoning": "JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©",
                "confidence": 0.5
            }
    
    def search_relevant_context(self, user_query: str, protocol: str, k: int = 5) -> List[Document]:
        """ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
        # í”„ë¡œí† ì½œë³„ í•„í„°ë§ì„ ìœ„í•œ ì¿¼ë¦¬ ìˆ˜ì •
        if protocol == "graphql":
            search_query = f"{user_query} GraphQL schema types queries mutations"
        else:
            search_query = f"{user_query} REST API OpenAPI operations endpoints"
        
        results = self.vectorstore.similarity_search(search_query, k=k)
        
        # í”„ë¡œí† ì½œë³„ í•„í„°ë§
        filtered_results = [
            doc for doc in results 
            if doc.metadata.get('type') == protocol
        ]
        
        return filtered_results
    
    def generate_graphql_query(self, user_query: str, context_docs: List[Document]) -> str:
        """GraphQL ì¿¼ë¦¬ ìƒì„±"""
        context_text = "\n\n".join([doc.page_content for doc in context_docs])
        
        response = self.llm.invoke(
            self.graphql_prompt.format(
                context=context_text,
                user_query=user_query
            )
        )
        
        return response.content.strip()
    
    def generate_rest_request(self, user_query: str, context_docs: List[Document]) -> str:
        """REST API ìš”ì²­ ìƒì„±"""
        context_text = "\n\n".join([doc.page_content for doc in context_docs])
        
        response = self.llm.invoke(
            self.rest_prompt.format(
                context=context_text,
                user_query=user_query
            )
        )
        
        return response.content.strip()
    
    def generate_query(self, user_query: str) -> Dict[str, Any]:
        """í†µí•© ì¿¼ë¦¬ ìƒì„±"""
        print(f"ğŸ” ì‚¬ìš©ì ìš”ì²­ ë¶„ì„: {user_query}")
        
        # í”„ë¡œí† ì½œ ê°ì§€
        protocol_result = self.detect_protocol(user_query)
        protocol = protocol_result["protocol"]
        confidence = protocol_result["confidence"]
        
        print(f"ğŸ“¡ ê°ì§€ëœ í”„ë¡œí† ì½œ: {protocol} (ì‹ ë¢°ë„: {confidence:.2f})")
        print(f"ğŸ’­ íŒë‹¨ ê·¼ê±°: {protocol_result['reasoning']}")
        
        # ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
        context_docs = self.search_relevant_context(user_query, protocol)
        print(f"ğŸ“š ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸: {len(context_docs)}ê°œ ë¬¸ì„œ")
        
        # ì¿¼ë¦¬ ìƒì„±
        if protocol == "graphql":
            generated_query = self.generate_graphql_query(user_query, context_docs)
        else:
            generated_query = self.generate_rest_request(user_query, context_docs)
        
        # ê²°ê³¼ êµ¬ì„±
        result = {
            "user_query": user_query,
            "detected_protocol": protocol,
            "confidence": confidence,
            "reasoning": protocol_result["reasoning"],
            "generated_query": generated_query,
            "context_sources": [
                {
                    "source": doc.metadata.get('source', 'Unknown'),
                    "type": doc.metadata.get('type', 'Unknown'),
                    "content_preview": doc.page_content[:200] + "..."
                }
                for doc in context_docs
            ]
        }
        
        return result

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import os
    import yaml
    
    # ì„¤ì • íŒŒì¼ ë¡œë“œ
    config_path = "config/settings.yaml"
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        openai_api_key = config.get('llm', {}).get('openai_api_key')
        rag_index_path = config.get('chroma', {}).get('persist_directory', 'rag/chroma_db')
        
        if not openai_api_key:
            print("âŒ ì„¤ì • íŒŒì¼ì—ì„œ OpenAI API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
    else:
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ í™•ì¸ (fallback)
        openai_api_key = os.getenv('OPENAI_API_KEY')
        rag_index_path = "rag/chroma_db"
        
        if not openai_api_key:
            print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
    
    print(f"âœ… OpenAI API í‚¤: ì„¤ì •ë¨")
    print(f"ğŸ“ ChromaDB ê²½ë¡œ: {rag_index_path}")
    
    # í†µí•© ì¿¼ë¦¬ ìƒì„±ê¸° ì´ˆê¸°í™”
    generator = IntegratedQueryGenerator(openai_api_key, rag_index_path)
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
    test_queries = [
        "ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤ ëª©ë¡ì„ ê°€ì ¸ì™€ì£¼ì„¸ìš”",
        "íŠ¹ì • ì‹œë‚˜ë¦¬ì˜¤ì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¡°íšŒí•˜ê³  ì‹¶ìŠµë‹ˆë‹¤",
        "ìƒˆë¡œìš´ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”",
        "ë³´ë“œ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ê³  ê´€ë ¨ ì‹œë‚˜ë¦¬ì˜¤ë„ í•¨ê»˜ ê°€ì ¸ì™€ì£¼ì„¸ìš”",
        "ì‹œë‚˜ë¦¬ì˜¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì¤‘ì§€í•´ì£¼ì„¸ìš”"
    ]
    
    print("ğŸš€ í†µí•© ì¿¼ë¦¬ ìƒì„±ê¸° í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    for i, query in enumerate(test_queries, 1):
        print(f"=== í…ŒìŠ¤íŠ¸ {i} ===")
        result = generator.generate_query(query)
        
        print(f"ğŸ“ ìƒì„±ëœ ì¿¼ë¦¬:")
        print(f"í”„ë¡œí† ì½œ: {result['detected_protocol']}")
        print(f"ì‹ ë¢°ë„: {result['confidence']:.2f}")
        print(f"ì¿¼ë¦¬:\n{result['generated_query']}")
        print(f"ì°¸ì¡° ì†ŒìŠ¤: {len(result['context_sources'])}ê°œ")
        print("-" * 50)
    
    # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ
    print("\nğŸ¯ ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ ì‹œì‘ (ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥)")
    while True:
        user_input = input("\nì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
            break
        
        if user_input:
            result = generator.generate_query(user_input)
            print(f"\nğŸ“¡ í”„ë¡œí† ì½œ: {result['detected_protocol']} (ì‹ ë¢°ë„: {result['confidence']:.2f})")
            print(f"ğŸ’­ íŒë‹¨ ê·¼ê±°: {result['reasoning']}")
            print(f"\nğŸ“ ìƒì„±ëœ ì¿¼ë¦¬:\n{result['generated_query']}")

if __name__ == "__main__":
    main() 